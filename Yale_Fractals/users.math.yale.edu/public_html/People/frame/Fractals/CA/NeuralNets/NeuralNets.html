<HTML>
<!-- Mirrored from users.math.yale.edu/public_html/People/frame/Fractals/CA/NeuralNets/NeuralNets.html by HTTrack Website Copier/3.x [XR&CO'2014], Thu, 30 Aug 2018 03:22:43 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=UTF-8" /><!-- /Added by HTTrack -->
<HEAD><TITLE>Fractal Geometry</TITLE></HEAD><BODY BGCOLOR="WHITE"><H1 ALIGN="JUSTIFY">4.K. Neural Nets</H1><P><TABLE><TR><TD>Artificial Intelligence (AI) has a long history of big breakthroughs being just over the 10 year horizon. Enthusiastic reports in the early 1960s predicted machine awareness by the early 1970s.  So far as we know, this has not happened yet.  Deep Blue notwithstanding, to date the most successful part of AI is neural nets, the part most directly inspired by biology.</TD></TR>   <TR><TD>A <FONT COLOR="RED">neural net</FONT> is a computational architecture based on the design of the interconnections of neurons in our brains.  There are many variations, but one of the simplest is a feed-forward net.</TD></TR><TR><TD ALIGN="CENTER"><IMG SRC="NNArch.gif" WIDTH=338 HEIGHT=288></TD></TR> <TR><TD>The input layer receives values, x<SUB>i</SUB> that it feeds forward to the hidden layer through the links indicated in the diagram. Associated with each link is a <FONT COLOR="RED">weight</FONT>, say w<SUB>i,j</SUB> is the weight of the link between the i<SUP>th</SUP> input neuron and the j<SUP>th</SUP> hidden neuron.  The input of the j<SUP>th</SUP> hidden neuron is the sum</TD></TR> <TR><TD ALIGN="CENTER">w<SUB>1,j</SUB>*x<SUB>1</SUB> + ... + w<SUB>L,j</SUB>*x<SUB>L</SUB></TD></TR><TR><TD>There are several schemes for how this input is handled by the hidden neuron.  The simplest is called a <FONT COLOR="RED">threshold function</FONT>.  If the weighted sum of the inputs exceeds a value, called the threshold, then the value of y<SUB>j</SUB> is set to 1; otherwise, it is set to 0.  The other common approach is called a <FONT COLOR="RED">sigmoid function</FONT>, pictured below.  Note as the weighted sum of the input values increases, the value of y<SUB>j</SUB> increases gradually, instead of abruptly with the threshold function.  (The threshold function more closely follows the behavior of biological neurons.) </TD></TR> <TR><TD ALIGN="CENTER"><IMG SRC="NNSigmoid.gif" WIDTH=462 HEIGHT=197></TD></TR> <TR><TD>Another set of weights, v<SUB>i,j</SUB>,connects the hidden neurons to the output neurons, and the same process feeds forward the values of the hidden neurons to the output neurons.  For example, given the network and weights pictured below, suppose the inputs are <NOBR>x<SUB>1</SUB> = 0</NOBR> and <NOBR>x<SUB>2</SUB> = 1</NOBR>.  What will the feedforward process give for the output neuron?</TD></TR><TR><TD ALIGN="CENTER"><IMG SRC="FeedForwEx.gif" WIDTH=305 HEIGHT=136></TD></TR> <TR><TD>First, compute the weighted inputs for the hidden neurons.</TD></TR>  <TR><TD ALIGN="CENTER"><TABLE><TR><TD>For y<SUB>1</SUB> the input is 2.5*0 + 1.2*1 = 1.2</TD></TR><TR><TD>For y<SUB>2</SUB> the input is -1.4*0 + 2.1*1 = 2.1</TD></TR></TABLE><TR><TD>Using a threshold function with a threshold value of 0, we see<NOBR>y<SUB>1</SUB> = 1</NOBR> and <NOBR>y<SUB>2</SUB> = 1</NOBR>.  Now the weighted input for the output neuron is</TD></TR> <TR><TD ALIGN="CENTER">-1.6*1 + 1.4*1 = -0.2</TD></TR><TR><TD>Consequently, the output value is z<SUB>1</SUB> = 0.</TD></TR> <TR><TD>So what?  The strength of neural nets lies not in their ability to compute in this fashion, but in their ability to <B>learn</B>, to <B>generalize</B>.  Neural nets can be trained.  Think of the training as learning to answer sample questions.  We want the net to produce specific outputs for certain inputs.  Training consists of adjusting the weights to match outputs and inputs.  Typically, there is a <FONT COLOR="RED">training set</FONT>, a collection of outputs paired to inputs.  The weights are adjusted so the first input produces the first output.  Next the weights are adjusted so the second input produces the second output.  This continues until the last input gives the last output.  By now, the weights have changed so much that the first input no longer produces the first output.  The training process is repeated through all the input-output pairs.  This is done again and again until the net gets them all right.</TD></TR>  <TR><TD>The remarkable thing about this process is that we have no idea of what the final weights mean.  But often the net generalizes its training set: it can correctly answer questions not in the training set.  We shall mention examples in a moment.</TD></TR>  <TR><TD>How are the weights adjusted to match the input-ouptut pairs?  One of the most common methods is <FONT COLOR="RED">back-propagation</FONT>.  The difference between the feedforward value of z<SUB>i</SUB> and the training set output value is the error, and the error is "propagated back" through the net, using the weights to compute errors at the hidden neurons, and ultimately to adjust the weights.</TD></TR>  <TR><TD>For example, suppose the training set for the net pictured above contains the input-output pair</TD></TR> <TR><TD ALIGN="CENTER">(x<SUB>1</SUB> = 0,  x<SUB>2</SUB> = 1; z<SUB>1</SUB> = 1)</TD></TR><TR><TD>The net picutured above is not trained for this input-output pair.  The output error is</TD></TR> <TR><TD ALIGN="CENTER">e<SUB>z<SUB>1</SUB></SUB> = 1 - 0 = 1</TD></TR><TR><TD>The weight between v<SUB>1,1</SUB> gives the error at y<SUB>1</SUB>:</TD></TR><TR><TD><TABLE><TR><TD>e<SUB>y<SUB>1</SUB></SUB> = e<SUB>z<SUB>1</SUB></SUB>*v<SUB>1,1</SUB> = 1*(-1.6) = -1.6</TD></TR><TR><TD>Similarly</TD></TR><TR><TD>e<SUB>y<SUB>2</SUB></SUB> = e<SUB>z<SUB>1</SUB></SUB>*v<SUB>2,1</SUB> = 1*1.4 = 1.4</TD></TR></TABLE></TD></TR><TR><TD>With these error values, we compute the changes in the weights.</TD></TR><TR><TD><TABLE><TR><TD>dv<SUB>1,1</SUB> = e<SUB>z<SUB>1</SUB></SUB>*y<SUB>1</SUB> = 1*1 = 1</TD></TR><TR><TD>dv<SUB>2,1</SUB> = e<SUB>z<SUB>1</SUB></SUB>*y<SUB>2</SUB> = 1*1 = 1</TD></TR><TR><TD>dw<SUB>1,1</SUB> = e<SUB>y<SUB>1</SUB></SUB>*x<SUB>1</SUB> = -1.6*0 = 0</TD></TR><TR><TD>dw<SUB>2,1</SUB> = e<SUB>y<SUB>1</SUB></SUB>*x<SUB>2</SUB> = -1.6*1 = -1.6</TD></TR><TR><TD>dw<SUB>1,2</SUB> = e<SUB>y<SUB>2</SUB></SUB>*x<SUB>1</SUB> = 1.4*0 = 0</TD></TR><TR><TD>dw<SUB>2,2</SUB> = e<SUB>y<SUB>2</SUB></SUB>*x<SUB>1</SUB> = 1.4*1 = 1.4</TD></TR></TABLE></TD></TR><TR><TD>Now new weights are computed.</TD></TR><TR><TD><TABLE><TR><TD>v<SUB>1,1</SUB> -> v<SUB>1,1</SUB> + dv<SUB>1,1</SUB> = -1.6 + 1 = -0.6</TD></TR><TR><TD>v<SUB>2,1</SUB> -> v<SUB>2,1</SUB> + dv<SUB>2,1</SUB> = 1.4 + 1 = 2.4</TD></TR><TR><TD>w<SUB>1,1</SUB> -> w<SUB>1,1</SUB> + dw<SUB>1,1</SUB> = 2.5 + 0 = 2.5</TD></TR><TR><TD>w<SUB>2,1</SUB> -> w<SUB>2,1</SUB> + dw<SUB>2,1</SUB> = 1.2 + (-1.6) = -0.4</TD></TR><TR><TD>w<SUB>1,2</SUB> -> w<SUB>1,2</SUB> + dw<SUB>1,2</SUB> = -1.4 + 0 = -1.4</TD></TR><TR><TD>w<SUB>2,2</SUB> -> w<SUB>2,2</SUB> + dw<SUB>2,2</SUB> = 2.1 + 1.4 = 3.5</TD></TR></TABLE></TD></TR><TR><TD>Here is the new net.  It is easy to verify this net is trained for the input-output pair (x<SUB>1</SUB> = 0,  x<SUB>2</SUB> = 1; z<SUB>1</SUB> = 1).</TD></TR> <TR><TD><IMG SRC="BackProp.gif" WIDTH=305 HEIGHT=136></TD></TR> <TR><TD>Here is a more interesting example, generated using <B>BrainMaker</B>, a commercial neural net package.  The goal is to teach the net to recognize digits presented as an 8 by 8 pixel array.  This net has 64 input neurons, 10 output neurons (one for each digit), and 25 hidden neurons.  The weights are randomized initially.  For example, the input 0 produces the output shown here. The darkness of the box indicates the "certainty" the net has of the value of the digit.  This net is reasonably confused: it is quite sure 0 is 0, 5, and 6.  Click on the picture to see the initial outputs for a this randomized net.</TD></TR><TR><TD><A HREF="NNDigits.gif"><IMG SRC="NNDigitsStill.gif" WIDTH=188 HEIGHT=179></A></TD></TR> <TR><TD>After training, the net will recognize the test digits, but what happens if we present it with slightly modified test digits?  Top left is the trianed response for the digit 9.  Note how the net's interpretations of the pixel pattern changes as we go step by step   from 8 to 9.  The patterns for 8 and 9 are quite similar, so it is little surprise that small changes destroy the net's certainty.</TD></TR><TR><TD><IMG SRC="NNGeneralize1.gif" WIDTH=405 HEIGHT=387></TD></TR> <TR><TD>On the other hand, here is the net's trained response to the pixel pattern for 0, and several variants.  No other pattern resembles 0 very much, so the net is able to generalize.  Small changes from the pattern of 0 it still recognizes as 0.  With enough changes, the net begins to recognize 3.</TD></TR>  <TR><TD><IMG SRC="NNGeneralize2.gif" WIDTH=405 HEIGHT=595></TD></TR> <TR><TD>Although the training process is completely deterministic, the initial weight space is so high-dimensional that predicting the outcome of training the randomized initial net is hopeless.  Adding to the complications is the observation that  usually there are many different combinations of weights that satisfy all the training set.  Do the basins of attraction have fractal boundaries?  We do not know.</TD></TR><TR><TD>Neural nets have many practical applications.  Perhaps one of the most surprising is landing large jet airliners.  Boeing and Airbus both have neural nets trained to land their largest planes.  The nets can assimilate much more information more rapidly than a human pilot, and if it has been trained for hundreds of hours in conditions like those used to train people, we would expect it to perform well.  How well is a matter of some disagreement, best summarized by this observation.  In a difficult landing, the Boeing pilot can override the neural net, whereas the Airbus net cannot be overriden.  Scary?  Just wait.</TD></TR></TABLE></P>    <P>Return to <A HREF="../welcome.html">Cellular Autimata</A><P ALIGN="JUSTIFY"></BODY>
<!-- Mirrored from users.math.yale.edu/public_html/People/frame/Fractals/CA/NeuralNets/NeuralNets.html by HTTrack Website Copier/3.x [XR&CO'2014], Thu, 30 Aug 2018 03:22:55 GMT -->
</HTML>